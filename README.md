# Bias-in-toxicity-classification
We write an algorithm that detects and removes unintended bias in that occurs in toxic comment classification. For instance, when we train for classification of toxic comments, the network may learn to associate toxicity with certain non-toxic features. For instance, because the word 'gay' is used frequently in a toxic comment and less in a positive or neutral comment, the network may mistakenly learn to associate the word 'gay' with toxicity so that when it encounters a neutral statement like 'I am a gay man', it may mistakenly identify the comment as toxic. This algorithm seeks to address this problem. The challenge is a Kaggle competition - https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification.   
